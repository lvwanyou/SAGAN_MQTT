%On the one hand, on account of directly calculating the relationship between any two hexadecimal characters to obtain the global geometric features in sequences, self-attention makes the extracted features more accurate in one step. On the other hand, the number of layers is strictly limited in the case of ensuring the accuracy to reduce the size of the model. 
\section{Conclusions and Future works}

%Fuzzy test based on deep adversarial learning is of great practical significance to the security verification of ICP. 
This paper applies deep adversarial learning from a self-attention perspective to generate fake but plausible fuzzing protocol messages of ICPs. We combine Wasserstein distance with self-attention to the model, making the model training more stable and computing hidden relationship between any two hexadecimal characters in parallel for all the input and output. And gradient penalty is applied to enforce the 1-Lipschitz constraint, which maintains the diversity of the generated data. The performance of our method is verified on datasets of different data sizes: the accuracy of the datasets containing 10,000 and 500,000 training data are 86.08\% and 92.71\% respectively, and F-measure are 82.08\% and 85.02\% respectively.

Our work motivates many avenues for future research, such as: GAN Compression \cite{li2020gan}, online learning with being deployed in embedded devices and anti-random strategies to improve the probability of anomalies triggering.
% Considering the current situation, we intend to perform the study in the following aspects in future studies. First, GAN Compression \cite{li2020gan} is a good way to keep the model more lightweight. And it is a promising application scenario that the lightweight model with online learning capabilities is deployed in embedded devices, which can learn protocol specifications or message formats of different protocols automatically. Second, a series of anti-random strategies after model training to improve the probability of anomalies triggering. Finally, we plan to apply the improved WGAN for more complex unsupervised learning NLP tasks including stateful protocols and non-stateful protocols. 



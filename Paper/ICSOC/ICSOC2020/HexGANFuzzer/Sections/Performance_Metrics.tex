\section{Performance Metrics}
Some quantitative criteria \cite{heusel2017gans} \cite{lucic2018gans} %\cite{karras2017progressive} 
have emerged recently for evaluating the performance of GAN on image generation. However, there is no corresponding evaluation metrics in fuzzing based on deep adversarial learning, and the lack of these indicators includes two aspects: the evaluation of the performance of the machine learning model \cite{karras2017progressive} in the field of fuzzing based on deep learning \cite{shmelkov2018good} and the evaluation of the vulnerability detection capability. Therefore, in accordance with our research purpose and specific situation, the following evaluation metrics are proposed in this study.


%represents the percentage of correctly generating characters about function codes in generated test case sequences versus function codes in real test case packages
\subsection{\textbf{F-measure}}
In order to further demonstrate the performance of our model in this paper, we compare the performance of different models on test data generation of ICPs with several indexs such as \textit{Sensitivity}, \textit{Specificity}, \textit{Accuracy} and \textit{F-measure}. Among them, Sensitivity represents the percentage of function codes for correctly generated test case packages versus function codes for real test case packages. Specificity means the percentage of non-essential hexadecimal characters for correctly generated test case packages versus non-essential hexadecimal characters for real test case packages. Accuracy is the percentage of the entire test case that correctly generates the format and message content of the test case package, and F-measure represents the harmonic mean of Precision and Sensitivity values directly. The specific formula is as follows:
\begin{equation}
Accuracy = \frac{{TP + TN}}{{TP + FN + TN + FP}}
\end{equation}
\begin{equation}
Sensitivity = \frac{{TP}}{{TP + FN}}
\end{equation}
\begin{equation}
Specificity = \frac{{TN}}{{TN + FP}}
\end{equation}
\begin{equation}
Precision = \frac{{TP}}{{TP + FP}}
\end{equation}
\begin{equation}
\begin{array}{l}
F-measure =\displaystyle{2 \times Precision \times} \displaystyle\frac{{Sensitivity}}{{Precision + Sensitivity}}
\end{array}
\end{equation}
wherein, TP/FP is the number of correctly/wrongly generated hexadecimal characters that constitute the function codes of ICPs' packages, and TN/FN is the number of correctly/wrongly generated other non-essential hexadecimal characters including message content of ICPs' packages.
%, FP is the number of hexadecimal characters  generated by error that constitute the function codes of ICPs' packages, and FN is the number of generated by error other non-essential hexadecimal characters including message content of ICPs' packages.


\subsection{\textbf{Effectiveness of Vulnerability Detection (EVD)}}
EVD refers to the ability to trigger anomalies on the basis of the test target accepting the test data. The effectiveness of fuzzing depends predominantly on the testing depth and high code coverage. Therefore, we propose the effectiveness evaluation metric EVD for fuzzing models from the above two aspects.
%  The effectiveness evaluation of the fuzzing methods can be divided into two aspects: testing depth and code coverage.

When the generated data can be accepted by the test target, it indicates that the generated data is similar to real data in terms of the format. This phenomenon reflects a certain depth of testing. But our ultimate goal is to trigger as many anomalies as possible to the ICS to find vulnerabilities, so two sub-metrics are taken into account when we finally calculate the testing depth for EVD: \textit{Acceptance Rate of Test Cases (ARTC)} and \textit{Ability of Vulnerability Detected (AVD)}. \textit{\textbf{ARTC}} is the quotient of the total number of test cases accepted and the total number of test cases sent. It refers to the acceptance rate at which the generated test data is received by the test target. \textit{\textbf{AVD}} is the quotient of the total number of test cases which trigger anomalies and the total number of test cases sent. It refers to the ability of triggering anomalies for the test target. 

\textit{Diversity of Generated Cases (DGC)} is a sub-metric proposed as the extent of code coverage for fuzzing in EVD. It refers to the ability to maintain the diversity of the generated data \cite{mondal2015exploring}. We define DGC as the quotient of the total number of message categories in the generated data frames and the total number of message categories in the training dataset.%The diversity-based approach is a useful test case selection criterion for code coverage \cite{hemmati2013achieving} . Furthermore, more diverse generated test data frames are likely to cause more exceptions. And this indicator focuses on the number of message types in the generated data.

In order to balance the different weights of sub-metrics on model effectiveness, dimensionless quantity method is applied to eliminate the influence of different scales between sub-metrics so that each sub-metric can be converted into a value that can be directly added or subtracted. We normalize aforementioned sub-metrics and add them up to get the comprehensive scores for EVD. We define $Norm$ as follows:

\begin{equation}
Norm(sm) = \frac{sm_i - sm_{min}}{sm_{max}-sm_{min}}
\end{equation}
where $sm$ is a sub-metric, $sm_i$ is the sub-metric value of the $ith$ model, ${{sm}_{max}}$ and ${{sm}_{min}}$ are respectively the maximum and minimum values of the corresponding sub-metric. And we define $EVDS=\{ARTC, AVD, DGC\}$, and:

\begin{equation}
EVD = \frac{1}{m}\sum_{j}^{m}Norm(EVDS_j)\times100\%
\end{equation}

%(such as error code 3, no response from the server, etc.) 
\subsection{\textbf{Efficiency of Vulnerability Detection (EFVD)}}
EFVD includes two sub-metrics, which are \textit{Training Time (TT)} and \textit{Time of Anomalies Triggered (TAT)}. \textit{\textbf{TT}} refers to the time required to train a model. Short TT correspondingly improves the efficiency of testing. \textit{\textbf{TAT}} refers to the time interval from the first request time (${t_1}$) to the time when the $i_{th}$ anomaly is triggered. Here we set it to 3, getting ${t_{{3_{{\rm{th\_anomaly}}}}}}$. 

It should be noted that the number of anomalies found is also related to the test target. Weak target will highlight the method effectiveness. However, because our ultimate goal is not to find the differences between the various test target, we only focus on the efficiency of the method in this study. We define $EFVDS=\{TT, TAT\}$. The specific formula is as follows:

\begin{equation}
EFVD = \frac{1}{m}\sum_{j}^{m}(1-Norm(EFVDS_j))\times100\%
\end{equation}

%
%\subsection{\textbf{Effectiveness of Vulnerability Detection (EVD)}}
%EVD refers to the ability to trigger anomalies on basis of the test target accepting the test data. The effectiveness of fuzzing depends predominantly on the testing depth and high code coverage. Therefore, we propose the effectiveness evaluation metric EVD for fuzzing models from the above two aspects.
%%  The effectiveness evaluation of the fuzzing methods can be divided into two aspects: testing depth and code coverage.
%
%When the generated data can be accepted by the test target, it indicates that the generated data is similar to real data in terms of the format, embodying that the model has the ability to learn the format of data frames from the real world. This phenomenon also reflects a certain depth of testing. But our ultimate goal is to trigger as many anomalies as possible to the ICS to find vulnerabilities, so two sub-metrics are taken into account when we finally calculate the testing depth for EVD: \textit{Acceptance Rate of Test Cases (ARTC)} and \textit{Ability of Vulnerability Detected (AVD)}:
%
%\textit{\textbf{a. ARTC.}} ARTC refers to the acceptance rate at which the generated test data is received by the test target when it is sent to the test target.
%ARTC is the quotient of the total number of test cases accepted and the total number of test cases sent.
%% The specific formula is as follows:
%%\begin{equation}
%%ARTC = \frac{{N_{accepted}}}{{N_{all}}} \times 100\%  
%%\end{equation}
%%where ${N_{accepted}}$ is the total number of test cases accepted and ${N_{all}}$ is the total number of test cases sent.
%
%\textit{\textbf{b. AVD.}} AVD refers to the ability of triggering anomalies for the test target when the generated test data is sent to the test target. We define AVD as the quotient of the total number of test cases which trigger anomalies and the total number of test cases sent.
%%\begin{equation}
%%AVD = \frac{{N_{anomalies}}}{{N_{all}}} \times 100\%  
%%\end{equation}
%%where ${N_{anomalies}}$ is the total number of test cases which trigger anomalies and ${N_{all}}$ is the total number of test cases sent.
%
%\textit{\textbf{Diversity of Generated Cases (DGC)}} is a sub-metric proposed as the extent of code coverage for fuzzing in EVD. It refers to the ability to maintain the diversity of the generated data \cite{hemmati2013achieving}. %The diversity-based approach is a useful test case selection criterion for code coverage \cite{hemmati2013achieving} \cite{mondal2015exploring}. Furthermore, more diverse generated test data frames are likely to cause more exceptions. And this indicator focuses on the number of message types in the generated data.
%We define DGC as the quotient of the total number of message categories in the generated data frames and the total number of message categories in the training dataset.
%%\begin{equation}
%%DGC = \frac{{N_{gen\_categories}}}{{N_{all\_categories}}} \times 100\% 
%%\end{equation}
%%where $N_{gen\_categories}$ is the total number of message categories in the generated data frames, and $N_{all\_categories}$ is the total number of message categories in the training dataset.
%
%In order to balance the different weights of sub-metrics on model effectiveness, dimensionless quantity method is applied to eliminate the influence of different scales between sub-metrics so that each sub-metric can be converted into a value that can be directly added or subtracted. We normalize aforementioned sub-metrics and add them up to get the comprehensive scores for EVD:
%\begin{equation}
%\begin{array}{l}
%EVD=(\displaystyle\frac{{ART{C_i} - ART{C_{\min }}}}{{ART{C_{\max }} - ART{C_{\min }}}} + \frac{{VD{E_i} - VD{E_{\min }}}}{{VD{E_{\max }} - VD{{E}_{\min }}}}\\
%\\{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}
%+ {\kern 1pt} \displaystyle\frac{{DG{C_i} - DG{C_{\min }}}}{{DG{C_{\max }} - DG{C_{\min }}}}) \times \displaystyle\frac{{100 \% }}{3}{\kern 1pt}
%\end{array}
%\end{equation}
%setting the values of different effectiveness sub-metrics of a model as \{${{sm}_1}$, ${{sm}_2}$, ... , ${{sm}_n}$\}, then values of ARTC, AVD and DGC of the ith model are: ${ARTC_i}$, ${AVD_i}$ and ${DGC_i}$. ${{sm}_{max}}$ and ${{sm}_{min}}$ are respectively the maximum and minimum values of the corresponding sub-metric.
%
%\subsection{\textbf{Efficiency of Vulnerability Detection (EFVD)}}
%EFVD is an important metric of model Efficiency. On one hand, it refers to the \textit{Training Time (TT)} required to train a model. On the other hand, it indicates the \textit{Time of Anomalies Triggered (TAT)} after sending a fixed number of test cases. 
%
%\quad \textit{\textbf{a. TT.}} Short TT correspondingly improves the efficiency of testing.
%
%% for each fuzzy test 
%\quad \textit{\textbf{b. TAT.}} TAT refers to the time interval from the first request time (${t_1}$) to the time when the third anomaly (such as error code 3, no response from the server, etc.) is triggered (${t_{{3_{{\rm{th\_anomaly}}}}}}$) . The following formula is for TAT in this paper:
%\begin{equation}
%TAT={t_{{3_{{\rm{th\_anomaly}}}}}} - {t_1}  
%\end{equation}
%It should be noted that the number of anomalies found is also related to the test target. Weak target will highlight the method effectiveness. However, because our ultimate goal is not to find the differences between the various test target, we only focus on the efficiency of the method in this study. The specific formula is as follows:
%
%\begin{equation}
%\begin{array}{l}
%EFVD = \displaystyle[(1 - \frac{{T{T_i} - T{T_{\min }}}}{{T{T_{\max }} - T{T_{\min }}}}) + \displaystyle(1 - \frac{{TA{T_i} - TA{T_{\min }}}}{{TA{T_{\max }} - TA{T_{\min }}}})]  
%\times  \displaystyle\frac{{100 \% }}{2}
%\end{array}
%\end{equation}
%similar to aforementioned effectiveness sub-metrics, the values of TT and TAT of the ith model are: ${TT_i}$ and ${TAT_i}$. ${{TT}_{max}}$ and ${{TAT}_{min}}$ are respectively the maximum and minimum values of the corresponding sub-metric.
